<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Vision to Language Survey by VisionToLanguageTeam</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Vision to Language Survey</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey" class="btn">View on GitHub</a>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="1-introduction" class="anchor" href="#1-introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>1. Introduction</strong>
</h2>

<p>The past year has seen an explosion in work on vision to language. In the 2012 ImageNet competition, a neural network model (`SuperVision') won at the task of object  classification.
In the 2013 ImageNet competition, a neural network model pioneered by UC Berkeley won at the task of object detection.
This unified two otherwise disparate computer vision fields under the same general modelling technique.  It became possible to train object detectors reliably, when provided with enough training data.</p>

<h2>
<a id="2-video-captioning" class="anchor" href="#2-video-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>2. Video Captioning</strong>
</h2>

<ul>
<li>
<p><strong>Robocup Soccer Dataset</strong> (UT Austin)</p>

<ul>
<li><p>There have been many efforts on automatic sportscasting of simulated soccer games. The most recent dataset for training purposes in this domain is a multilingual corpus of Robocup soccer events aligned with human-generated comments in Korean and English.</p></li>
<li><p>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language.
David L. Chen, Joohyun Kim, Raymond J. Mooney.
In Journal of Artificial Intelligence Research (JAIR) , 37, pages 397-435, 2010.
<a href="http://www.cs.utexas.edu/%7Eml/clamp/sportscasting/">[<strong>Project Page</strong>]</a>
<a href="http://dl.acm.org/citation.cfm?id=1861761">[ACM]</a>
<a href="https://www.jair.org/media/2962/live-2962-4903-jair.pdf">[PDF]</a>
<a href="http://www.jair.org/papers/paper2962.html">[JAIR link]</a>    </p></li>
</ul>
</li>
<li>
<p><strong>Video-with-Sentences Dataset</strong> (Purdue University)</p>

<ul>
<li><p>This work provides a dataset for learning word meanings from short video clips which are manually annotated with one or more sentences. Their dataset manually annotates 3-5 second long 61 video clips with sentences which are highly resctricted in terms of grammar and language.</p></li>
<li><p>Grounded Language Learning from Video Described with Sentences.
H. Yu and J. M. Siskind.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013, <em>best paper award</em>.
<a href="http://haonanyu.com/research/acl2013/">[<strong>Project Page</strong>]</a>
<a href="http://haonanyu.com/wp-content/uploads/2013/05/yu13.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>





<ul>
<li>
<p><strong>Multilingual Parallel Video Description Dataset</strong> (UT Austin &amp; MSR)</p>

<ul>
<li><p>One sentence here.</p></li>
<li><p>Collecting Highly Parallel Data for Paraphrase Evaluation.
David L. Chen and William B. Dolan.
Annual Meetings of the Association for Computational Linguistics (ACL), 2011.
<a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/">[<strong>Project Page</strong>]</a>
<a href="http://www.cs.utexas.edu/users/ml/papers/chen.acl11.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>How-To Video Captions</strong></p>

<ul>
<li><p>One sentence here.</p></li>
<li><p>What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy.
NAACL 2015.
<a href="http://www.cs.ubc.ca/%7Emurphyk/Papers/naacl15.pdf">[PDF]</a></p></li>
<li><p>Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments.
I. Naim, Y. Song, Q. Liu, L. Huang, H. Kautz, J. Luo, and D. Gildea. 
Proc. NAACL 2015.
<a href="http://acl.cs.qc.edu/%7Elhuang/papers/naim-video.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Saarbrucken Corpus of Textually Annotated Scenes (TACOS Corpus)</strong> (Saarland University &amp;  Max Planck Institute for Informatics)</p>

<ul>
<li><p>One sentence here.</p></li>
<li><p>Grounding Action Descriptions in Videos.
Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.
TACL 2013.
<a href="http://www.aclweb.org/anthology/Q13-1003">[PDF]</a></p></li>
</ul>
</li>
</ul>





<h2>
<a id="3-image-captioning" class="anchor" href="#3-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>3. Image Captioning</strong>
</h2>

<h3>
<a id="user-generated-captions" class="anchor" href="#user-generated-captions" aria-hidden="true"><span class="octicon octicon-link"></span></a>User-generated Captions</h3>

<ul>
<li>
<p><strong>SBU Captioned Photo Dataset</strong> (Stony Brook University)</p>

<ul>
<li><p>One sentence here.</p></li>
<li><p>Im2Text: Describing Images Using 1 Million Captioned Photograph.
Vicente Ordonez, Girish Kulkarni, Tamara L. Berg. s.
Neural Information Processing Systems(NIPS), 2011.
<a href="http://tlberg.cs.unc.edu/vicente/sbucaptions/">[<strong>Project Page</strong>]</a>
<a href="http://tamaraberg.com/papers/generation_nips2011.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Flickr's 100 million images dataset 2012 (YFCC100M)</strong></p>

<ul>
<li><p>One sentence here.</p></li>
<li><p><a href="http://labs.yahoo.com/news/yfcc100m/">[<strong>Project Page</strong>]</a></p></li>
</ul>
</li>
<li>
<p><strong>Déjà-Image Captions Dataset</strong></p>

<ul>
<li><p>The deja-image captions data set consists of naturally occurring captions for images where the captions were found to be nearly identical for multiple images; the language is rich and varied, with 17 percent in a random sample found to be figurative. Each image is augmented with a set of retrieved candidates from the full set, that were vetted by AMT workers to also be good captions for the image.</p></li>
<li><p>Déjà Image-Captions: A Corpus of Expressive Image Descriptions in Repetition.
Jianfu Chen, Polina Kuznetsova, David Warren, Yejin Choi.
North American Chapter of the Association for Computational Linguistics (NAACL), 2015.
<a href="http://nlclient83.cs.stonybrook.edu:8081/static/index.html">[<strong>Project Page</strong>]</a>
<a href="http://www3.cs.stonybrook.edu/%7Ejianchen/papers/naacl2015.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>

<h3>
<a id="crowd-sourced-captions" class="anchor" href="#crowd-sourced-captions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Crowd-sourced Captions</h3>

<ul>
<li><p><strong>PASCAL Sentences Dataset (1K)</strong></p></li>
<li>
<p><strong>Flickr 8K</strong> (UIUC)</p>

<ul>
<li><p>This dataset consists of 8,092 Flickr images, each captioned by multiple Amazon Turkers totalling more than 40,000 image description. The focus of the dataset is on people or animals (mainly dogs) performing some specific action. </p></li>
<li><p>Collecting Image Annotations Using Amazon's Mechanical Turk.
Cyrus Rashtchian, Peter Young, Micah Hodosh and Julia Hockenmaier.
Proc. the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk.
<a href="http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html">[<strong>Project Page</strong>]</a></p></li>
</ul>
</li>
<li>
<p><strong>Flickr30K</strong> (UIUC)</p>

<ul>
<li><p>This dataset is an extention of Flickr 8K dataset consisting of 158,915 crowd-sourced captions which describe 31,783 images. This dataset mainly focuses on people performing everyday activities and involved in everyday events.</p></li>
<li><p>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.
Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier.
Transactions of the Association for Computational Linguistics 2 (2014): 67-78.
<a href="http://shannon.cs.illinois.edu/DenotationGraph/">[<strong>Project Page</strong>]</a>
<a href="http://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Flickr30K Entities</strong> (UIUC)</p>

<ul>
<li><p>One sentence here.</p></li>
<li><p>Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.
Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
arXiv:1505.04870, 2015.
<a href="http://web.engr.illinois.edu/%7Ebplumme2/Flickr30kEntities/">[<strong>Project Page</strong>]</a>
<a href="http://arxiv.org/pdf/1505.04870v1.pdf">[PDF]</a>
<a href="http://arxiv.org/abs/1505.04870">[Arxiv]</a></p></li>
</ul>
</li>
<li>
<p><strong>Microsoft Research Dense Visual Annotation Corpus</strong> (Microsoft Research)</p>

<ul>
<li><p>One sentence here.</p></li>
<li><p>See No Evil, Say No Evil: Description Generation from Densely Labeled Images.
Mark Yatskar, Michel Galley, Lucy Vanderwende, and Luke Zettlemoyer.
In Third Joint Conference on Lexical and Computation Semantics (*SEM) , 2014.
<a href="http://research.microsoft.com/en-us/downloads/b8887ebe-dc2f-4f4b-94d4-65b8432f7df4/">[<strong>Project Page</strong>]</a>
<a href="http://homes.cs.washington.edu/%7Emy89/">[Code and Data]</a>
<a href="http://homes.cs.washington.edu/%7Emy89/publications/StarSem2014-SeeNoEvil.pdf">[PDF]</a></p></li>
</ul>
</li>
<li><p><strong>COCO Dataset</strong> (Microsoft Research)</p></li>
<li>
<p><strong>Visual and Linguistic Treebank</strong> (University of Edinburgh)</p>

<ul>
<li><p>The Visual and Linguistic Treebank consists of a set of 2,424 images with 3 descriptions for each image sourced by asking AMT workers to write one sentence describing the main action in the photo (there are only 10 types of actions in the set) and one sentence describing the other regions not involved in the action. For a set of 341 images, AMT workers additionally identified the bounding boxes of the objects in the description and created the visual dependency representation for each image, which represents the relative positions of the various objects in the image.</p></li>
<li><p>Image Description using Visual Dependency Representations.
Desmond Elliott and Frank Keller.
EMNLP 2013.
<a href="http://aclweb.org/anthology/D/D13/D13-1128.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>

<h2>
<a id="4-beyond-image-captioning" class="anchor" href="#4-beyond-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>4. Beyond Image Captioning</strong>
</h2>

<ul>
<li>
<p><strong>Visual MadLibs</strong></p>

<ul>
<li><p>One sentence here</p></li>
<li><p>Visual Madlibs: Fill in the blank Image Generation and Question Answering.
Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg.
arXiv:1506.00278 [cs.CV].
<a href="http://arxiv.org/abs/1506.00278">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1506.00278.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>ReferIt dataset</strong></p>

<ul>
<li><p>One sentence here</p></li>
<li><p>ReferItGame: Referring to Objects in Photographs of Natural Scenes.
Sahar Kazemzadeh<em>, Vicente Ordonez</em>, Mark Matten, Tamara L. Berg.
Empirical Methods in Natural Language Processing (EMNLP) 2014.  Doha, Qatar.  October 2014.
<a href="http://tamaraberg.com/referitgame/">[<strong>Project Page</strong>]</a>
<a href="http://tamaraberg.com/papers/referit.pdf">[PDF]</a>
<a href="http://www.referitgame.com/">[Game]</a></p></li>
</ul>
</li>
<li>
<p><strong>Visual Question Answering (VQA) Dataset</strong> (Microsoft Research)</p>

<ul>
<li><p>One sentence here</p></li>
<li><p>VQA: Visual Question Answering.**
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh.
arXiv:1505.00468 [cs.CL]
<a href="http://arxiv.org/abs/1505.00468">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1505.00468v1.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>



<ul>
<li>
<p><strong>COCO-QA Dataset</strong> (University of Toronto)</p>

<ul>
<li><p>This is a simpler VQA dataset where the questions are automatically generated from image captions of MS COCO dataset. This dataset has total 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations.</p></li>
<li><p>Image Question Answering: A Visual Semantic Embedding Model and a New Dataset.
Mengye Ren, Ryan Kiros, Richard Zemel.
arXiv:1505.02074 [cs.LG].
<a href="http://arxiv.org/abs/1505.02074">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1505.02074v1.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>



<ul>
<li>
<strong>Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks.</strong>
Xiao Lin, Devi Parikh.
arXiv:1502.06108 [cs.CV].
<a href="http://arxiv.org/abs/1502.06108">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1502.06108v2.pdf">[PDF]</a>
</li>
</ul>

<h2>
<a id="5-more-possibilities" class="anchor" href="#5-more-possibilities" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>5. More Possibilities</strong>
</h2>

<ul>
<li>What, Where, Who? (ICCV 2007) \cite{fei2010whathwherewho}</li>
<li>Ramanath event-centric paper (with Fei Fei) (ICCV 2013)</li>
<li>Ontology of events and social settings (Karpathy and Fei Fei 2015)</li>
</ul>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey">Vision to Language Survey</a> is maintained by <a href="https://github.com/VisionToLanguageTeam">VisionToLanguageTeam</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

