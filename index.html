<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Vision to Language Survey by VisionToLanguageTeam</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Vision to Language Survey</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey" class="btn">View on GitHub</a>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="1-introduction" class="anchor" href="#1-introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>1. Introduction</strong>
</h2>

<p>The past year has seen an explosion in work on vision to language. In the 2012 ImageNet competition, a neural network model (`SuperVision') won at the task of object  classification.
In the 2013 ImageNet competition, a neural network model pioneered by UC Berkeley won at the task of object detection.
This unified two otherwise disparate computer vision fields under the same general modelling technique.  It became possible to train object detectors reliably, when provided with enough training data.</p>

<h2>
<a id="2-video-captioning" class="anchor" href="#2-video-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>2. Video Captioning</strong>
</h2>

<ul>
<li><p><strong>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language</strong>.
David L. Chen, Joohyun Kim, Raymond J. Mooney.
In Journal of Artificial Intelligence Research (JAIR) , 37, pages 397-435, 2010.
<a href="http://www.cs.utexas.edu/%7Eml/clamp/sportscasting/">[<strong>Project Page</strong>]</a>
<a href="http://dl.acm.org/citation.cfm?id=1861761">[ACM]</a>
<a href="https://www.jair.org/media/2962/live-2962-4903-jair.pdf">[PDF]</a>
<a href="http://www.jair.org/papers/paper2962.html">[JAIR link]</a></p></li>
<li><p><strong>Grounded Language Learning from Video Described with Sentences</strong>.
H. Yu and J. M. Siskind.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013, <em>best paper award</em>.
<a href="http://haonanyu.com/research/acl2013/">[<strong>Project Page</strong>]</a>
<a href="http://haonanyu.com/research/acl2013/">[PDF]</a></p></li>
<li><p><strong>Story-Driven Summarization for Egocentric Video</strong>.
Zheng Lu and Kristen Grauman.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Portland, OR, June 2013.
<a href="http://vision.cs.utexas.edu/projects/egocentric/storydriven.html">[<strong>Project Page</strong>]</a>
<a href="http://www.cs.utexas.edu/%7Egrauman/papers/lu-grauman-cvpr2013.pdf">[PDF]</a></p></li>
<li><p><strong>Movie Script Summarization as Graph-based Scene Extraction</strong>.
Philip John Gorinski and Mirella Lapata.
Proc. Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL (NAACL 2015), pages 1066–1076.
May 31 – June 5, 2015.
<a href="http://www.aclweb.org/anthology/N/N15/N15-1113.pdf">[PDF]</a></p></li>
<li><p><strong>Collecting Highly Parallel Data for Paraphrase Evaluation</strong>
David L. Chen and William B. Dolan.
Annual Meetings of the Association for Computational Linguistics (ACL), 2011.
<a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/">[<strong>Project Page</strong>]</a>
<a href="http://www.cs.utexas.edu/users/ml/papers/chen.acl11.pdf">[PDF]</a></p></li>
<li><p><strong>What’s Cookin’? Interpreting Cooking Videos using Text, Speech and Vision</strong>.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy.
NAACL 2015.
<a href="http://www.cs.ubc.ca/%7Emurphyk/Papers/naacl15.pdf">[PDF]</a></p></li>
<li><p><strong>Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments</strong>.
I. Naim, Y. Song, Q. Liu, L. Huang, H. Kautz, J. Luo, and D. Gildea. 
Proc. NAACL 2015.
<a href="http://acl.cs.qc.edu/%7Elhuang/papers/naim-video.pdf">[PDF]</a></p></li>
<li><p><strong>Grounding Action Descriptions in Videos</strong>.
Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.
TACL 2013.
<a href="http://www.aclweb.org/anthology/Q13-1003">[PDF]</a></p></li>
<li><p><strong>Translating Videos to Natural Language Using Deep Recurrent Neural Networks</strong>.
Subhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko.
North American Chapter of the Association for Computational Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)
<a href="https://www.cs.utexas.edu/%7Evsub/pdf/Translating_Videos_NAACL15.pdf">[PDF]</a>
<a href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube">[Code]</a></p></li>
<li><p><strong>Book2movie: Aligning video scenes with book chapters.</strong>
Tapaswi, Makarand, Martin Bäuml, and Rainer Stiefelhagen.
Proc. the IEEE Conference on Computer Vision and Pattern Recognition. 2015.
<a href="https://cvhci.anthropomatik.kit.edu/%7Emtapaswi/papers/CVPR2015.pdf">[PDF]</a></p></li>
</ul>

<h2>
<a id="3-image-captioning" class="anchor" href="#3-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>3. Image Captioning</strong>
</h2>

<h3>
<a id="user-generated" class="anchor" href="#user-generated" aria-hidden="true"><span class="octicon octicon-link"></span></a>User-generated</h3>

<ul>
<li><p><strong>SBU Captioned Photo Dataset</strong>: 
Im2Text: Describing Images Using 1 Million Captioned Photograph.
Vicente Ordonez, Girish Kulkarni, Tamara L. Berg. s.
Neural Information Processing Systems(NIPS), 2011.
<a href="http://tlberg.cs.unc.edu/vicente/sbucaptions/">[<strong>Project Page</strong>]</a>
<a href="http://tamaraberg.com/papers/generation_nips2011.pdf">[PDF]</a></p></li>
<li><p><strong>Flickr's 100 million images dataset 2012 (YFCC100M)</strong>
<a href="http://labs.yahoo.com/news/yfcc100m/">[<strong>Project Page</strong>]</a></p></li>
</ul>

<h3>
<a id="captioned-by-crowd" class="anchor" href="#captioned-by-crowd" aria-hidden="true"><span class="octicon octicon-link"></span></a>Captioned by Crowd</h3>

<ul>
<li><p><strong>PASCAL Sentences Dataset (1K)</strong></p></li>
<li><p><strong>Flickr 8K</strong>
<a href="http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html">[<strong>Project Page</strong>]</a></p></li>
<li><p><strong>Flickr30K</strong>
From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.
Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier.
Transactions of the Association for Computational Linguistics 2 (2014): 67-78.
<a href="http://shannon.cs.illinois.edu/DenotationGraph/">[<strong>Project Page</strong>]</a>
<a href="http://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf">[PDF]</a></p></li>
<li><p><strong>Flickr30K Entities</strong>:
Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.
Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
arXiv:1505.04870, 2015.
<a href="http://web.engr.illinois.edu/%7Ebplumme2/Flickr30kEntities/">[<strong>Project Page</strong>]</a>
<a href="http://arxiv.org/pdf/1505.04870v1.pdf">[PDF]</a>
<a href="http://arxiv.org/abs/1505.04870">[Arxiv]</a></p></li>
<li><p><strong>Microsoft Research Dense Visual Annotation Corpus</strong>:
See No Evil, Say No Evil: Description Generation from Densely Labeled Images.
Mark Yatskar, Michel Galley, Lucy Vanderwende, and Luke Zettlemoyer.
In Third Joint Conference on Lexical and Computation Semantics (*SEM) , 2014.
<a href="http://research.microsoft.com/en-us/downloads/b8887ebe-dc2f-4f4b-94d4-65b8432f7df4/">[<strong>Project Page</strong>]</a>
<a href="http://homes.cs.washington.edu/%7Emy89/">[Code and Data]</a>
<a href="http://homes.cs.washington.edu/%7Emy89/publications/StarSem2014-SeeNoEvil.pdf">[PDF]</a></p></li>
<li><p><strong>COCO Dataset</strong></p></li>
<li><p>Elliot &amp; Keller Visual Dependency Graph (VDG) work 
--&gt; \cite{elliott2013visualdependency}</p></li>
</ul>

<h3>
<a id="already-captioned" class="anchor" href="#already-captioned" aria-hidden="true"><span class="octicon octicon-link"></span></a>Already Captioned</h3>

<ul>
<li>Yejin's new dataset (expressive language) - NAACL 2015; Deja Image Captions
--&gt; \cite{chen2015deja}<br>
</li>
</ul>

<h2>
<a id="4-beyond-image-captioning" class="anchor" href="#4-beyond-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>4. Beyond Image Captioning</strong>
</h2>

<ul>
<li><p>Bergs' MadLibs (and QA?) <a href="http://arxiv.org/pdf/1506.00278.pdf">http://arxiv.org/pdf/1506.00278.pdf</a> 
\cite{yu2015vml}</p></li>
<li><p>Bergs' ReferIt dataset (EMNLP 2014) <a href="http://tamaraberg.com/referitgame/">http://tamaraberg.com/referitgame/</a>
\cite{kazemzadeh2014referitgame}</p></li>
<li><p>VQA (V. tech) - ICCV 2015 \cite{antol2015vqa}
    -- 10K sampled (somehow) from MS COCO</p></li>
<li><p>mQA (might not be released yet?) Baidu - 2015 - captions converted to QA <a href="http://arxiv.org/pdf/1505.05612.pdf">http://arxiv.org/pdf/1505.05612.pdf</a>, NIPS 2015</p></li>
<li><p>The "Image Question Answering" dataset from Toronto:
\cite{ren2015imageqa}</p></li>
<li><p>dataset from ("Joint Photo Stream and Blog Post Summarization and Exploration" and "Ranking and Retrieval of Image Sequences from Multiple Paragraph Queries"), CVPR 2015</p></li>
<li><p>Disney dataset (check that is same as above/hasn't changed)</p></li>
<li><p>Devi Parikh &amp; students: fill-in-the-blanks and visual paraphrasing (page under constructions, datasets and code will be online here: <a href="https://filebox.ece.vt.edu/%7Elinxiao/imagine/">https://filebox.ece.vt.edu/~linxiao/imagine/</a>), CVPR 2015 \cite{lin2015imagine}</p></li>
</ul>

<h2>
<a id="5-more-possibilities" class="anchor" href="#5-more-possibilities" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>5. More Possibilities</strong>
</h2>

<ul>
<li>What, Where, Who? (ICCV 2007) \cite{fei2010whathwherewho}</li>
<li>Ramanath event-centric paper (with Fei Fei) (ICCV 2013)</li>
<li>Ontology of events and social settings (Karpathy and Fei Fei 2015)</li>
</ul>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey">Vision to Language Survey</a> is maintained by <a href="https://github.com/VisionToLanguageTeam">VisionToLanguageTeam</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

