<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Vision to Language Survey by VisionToLanguageTeam</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Vision to Language Survey</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey" class="btn">View on GitHub</a>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="1-introduction" class="anchor" href="#1-introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>1. Introduction</strong>
</h2>

<p>The past year has seen an explosion in work on vision to language. In the 2012 ImageNet competition, a neural network model (`SuperVision') won at the task of object  classification.
In the 2013 ImageNet competition, a neural network model pioneered by UC Berkeley won at the task of object detection.
This unified two otherwise disparate computer vision fields under the same general modelling technique.  It became possible to train object detectors reliably, when provided with enough training data.</p>

<h2>
<a id="2-video-captioning" class="anchor" href="#2-video-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>2. Video Captioning</strong>
</h2>

<ul>
<li>
<p><strong>Robocup Soccer Dataset</strong> (UT Austin)</p>

<ul>
<li><p>There have been many efforts on automatic sportscasting of simulated soccer games. The most recent dataset for training purposes in this domain is a multilingual corpus of Robocup soccer events aligned with human-generated comments in Korean and English.</p></li>
<li><p>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language.
David L. Chen, Joohyun Kim, Raymond J. Mooney.
In Journal of Artificial Intelligence Research (JAIR) , 37, pages 397-435, 2010.
<a href="http://www.cs.utexas.edu/%7Eml/clamp/sportscasting/">[<strong>Project Page</strong>]</a>
<a href="http://dl.acm.org/citation.cfm?id=1861761">[ACM]</a>
<a href="https://www.jair.org/media/2962/live-2962-4903-jair.pdf">[PDF]</a>
<a href="http://www.jair.org/papers/paper2962.html">[JAIR link]</a>    </p></li>
</ul>
</li>
<li>
<p><strong>Video-with-Sentences Dataset</strong> (Purdue University)</p>

<ul>
<li><p>This work provides a dataset for learning word meanings from short video clips which are manually annotated with one or more sentences. Their dataset manually annotates 3-5 second long 61 video clips with sentences which are highly resctricted in terms of grammar and language.</p></li>
<li><p>Grounded Language Learning from Video Described with Sentences.
H. Yu and J. M. Siskind.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013, <em>best paper award</em>.
<a href="http://haonanyu.com/research/acl2013/">[<strong>Project Page</strong>]</a>
<a href="http://haonanyu.com/wp-content/uploads/2013/05/yu13.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>





<ul>
<li>
<p><strong>Multilingual Parallel Video Description Dataset</strong> (UT Austin &amp; MSR)</p>

<ul>
<li><p>Another major dataset is Microsoft Research's Video Description Corpus which is a highly parallel dataset for paraphrase evaluations. The goal of this corpus was to collect large numbers of paraphrases using a crowd workers. Basically turkers were instructed to watch a short video snippet (between 10 and 25 seconds) and then summarize the action or event in the video with a single sentence. The crowd-sourcing task resulted in parallel descriptions of 2,089 videos. In this dataset both paraphrase and bilingual alternatives are captured, hence, the dataset can be useful for training and testing translation, paraphrasing, and video description purposes.</p></li>
<li><p>Collecting Highly Parallel Data for Paraphrase Evaluation.
David L. Chen and William B. Dolan.
Annual Meetings of the Association for Computational Linguistics (ACL), 2011.
<a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/">[<strong>Project Page</strong>]</a>
<a href="http://www.cs.utexas.edu/users/ml/papers/chen.acl11.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>How-To Video Captions</strong></p>

<ul>
<li><p>Recently there have been few works on learning to integrate natural language descriptions with events in videos. Some earlier works had relied on videos which are pre-segmented to short chunks, where each video segment was manually aligned  with a natural language description. More recent works have proposed unsupervised learning algorithms for automatically associating sentences in a document with video segments. Naim et al. align the natural language instructions for biological experiments in `wet laboratories' with recorded videos of people performing these experiments. Malmaud et al. focus on the cooking domain, aligning written recipe steps with a videos.</p></li>
<li><p>What’s Cookin’? Interpreting Cooking Videos using Text, Speech and Vision.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy.
NAACL 2015.
<a href="http://www.cs.ubc.ca/%7Emurphyk/Papers/naacl15.pdf">[PDF]</a></p></li>
<li><p>Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments.
I. Naim, Y. Song, Q. Liu, L. Huang, H. Kautz, J. Luo, and D. Gildea. 
Proc. NAACL 2015.
<a href="http://acl.cs.qc.edu/%7Elhuang/papers/naim-video.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Saarbrucken Corpus of Textually Annotated Scenes (TACOS Corpus)</strong></p>

<ul>
<li><p>"MPII Cooking Composite Activities" is a video corpus that annotates 41 low-level different cooking activities (e.g., <code>separating eggs' or</code>cutting veggies') in 212 video segments with average 4.5 minutes length. This corpus specifically annotates participating objects in activities. The ''Saarbrucken Corpus of Textually Annotated Scenes'' (TACOS) is built on top of the MPII video corpus, on a subset that involves manipulation of cooking ingredients. They have extended the MPII corpus with multiple crow-sourced textual descriptions. They mainly align the sentences describing activities with their corresponding video segments. By adding textual descriptions and aligning them on the sentence level with videos and low-level annotations, TACOS can serves as a multimodal resource for various research topics. This dataset include 2206 textual video descriptions aligned with 127 videos.</p></li>
<li><p>Grounding Action Descriptions in Videos.
Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.
TACL 2013.
<a href="http://www.aclweb.org/anthology/Q13-1003">[PDF]</a></p></li>
</ul>
</li>
<li><p><strong>Translating Videos to Natural Language Using Deep Recurrent Neural Networks</strong>.
Subhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko.
North American Chapter of the Association for Computational Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)
<a href="https://www.cs.utexas.edu/%7Evsub/pdf/Translating_Videos_NAACL15.pdf">[PDF]</a>
<a href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube">[Code]</a></p></li>
<li><p><strong>Book2movie: Aligning video scenes with book chapters.</strong>
Tapaswi, Makarand, Martin Bäuml, and Rainer Stiefelhagen.
Proc. the IEEE Conference on Computer Vision and Pattern Recognition. 2015.
<a href="https://cvhci.anthropomatik.kit.edu/%7Emtapaswi/papers/CVPR2015.pdf">[PDF]</a></p></li>
</ul>

<h2>
<a id="3-image-captioning" class="anchor" href="#3-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>3. Image Captioning</strong>
</h2>

<h3>
<a id="user-generated-captions" class="anchor" href="#user-generated-captions" aria-hidden="true"><span class="octicon octicon-link"></span></a>User-generated Captions</h3>

<ul>
<li><p><strong>SBU Captioned Photo Dataset</strong>: 
Im2Text: Describing Images Using 1 Million Captioned Photograph.
Vicente Ordonez, Girish Kulkarni, Tamara L. Berg. s.
Neural Information Processing Systems(NIPS), 2011.
<a href="http://tlberg.cs.unc.edu/vicente/sbucaptions/">[<strong>Project Page</strong>]</a>
<a href="http://tamaraberg.com/papers/generation_nips2011.pdf">[PDF]</a></p></li>
<li><p><strong>Flickr's 100 million images dataset 2012 (YFCC100M)</strong>:
<a href="http://labs.yahoo.com/news/yfcc100m/">[<strong>Project Page</strong>]</a></p></li>
<li><p><strong>Déjà Image-Captions</strong>:
Déjà Image-Captions: A Corpus of Expressive Image Descriptions in Repetition.
Jianfu Chen, Polina Kuznetsova, David Warren, Yejin Choi.
North American Chapter of the Association for Computational Linguistics (NAACL), 2015.
<a href="http://nlclient83.cs.stonybrook.edu:8081/static/index.html">[<strong>Project Page</strong>]</a>
<a href="http://www3.cs.stonybrook.edu/%7Ejianchen/papers/naacl2015.pdf">[PDF]</a></p></li>
</ul>

<h3>
<a id="crowd-sourced-captions" class="anchor" href="#crowd-sourced-captions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Crowd-sourced Captions</h3>

<ul>
<li><p><strong>PASCAL Sentences Dataset (1K)</strong></p></li>
<li><p><strong>Flickr 8K</strong>
<a href="http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html">[<strong>Project Page</strong>]</a></p></li>
<li><p><strong>Flickr30K</strong>
From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.
Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier.
Transactions of the Association for Computational Linguistics 2 (2014): 67-78.
<a href="http://shannon.cs.illinois.edu/DenotationGraph/">[<strong>Project Page</strong>]</a>
<a href="http://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf">[PDF]</a></p></li>
<li><p><strong>Flickr30K Entities</strong>
Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.
Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
arXiv:1505.04870, 2015.
<a href="http://web.engr.illinois.edu/%7Ebplumme2/Flickr30kEntities/">[<strong>Project Page</strong>]</a>
<a href="http://arxiv.org/pdf/1505.04870v1.pdf">[PDF]</a>
<a href="http://arxiv.org/abs/1505.04870">[Arxiv]</a></p></li>
<li><p><strong>Microsoft Research Dense Visual Annotation Corpus</strong>
See No Evil, Say No Evil: Description Generation from Densely Labeled Images.
Mark Yatskar, Michel Galley, Lucy Vanderwende, and Luke Zettlemoyer.
In Third Joint Conference on Lexical and Computation Semantics (*SEM) , 2014.
<a href="http://research.microsoft.com/en-us/downloads/b8887ebe-dc2f-4f4b-94d4-65b8432f7df4/">[<strong>Project Page</strong>]</a>
<a href="http://homes.cs.washington.edu/%7Emy89/">[Code and Data]</a>
<a href="http://homes.cs.washington.edu/%7Emy89/publications/StarSem2014-SeeNoEvil.pdf">[PDF]</a></p></li>
<li><p><strong>COCO Dataset</strong></p></li>
<li><p><strong>Image Description using Visual Dependency Representations.</strong>
Desmond Elliott and Frank Keller.
EMNLP 2013.
<a href="http://aclweb.org/anthology/D/D13/D13-1128.pdf">[PDF]</a></p></li>
</ul>

<h2>
<a id="4-beyond-image-captioning" class="anchor" href="#4-beyond-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>4. Beyond Image Captioning</strong>
</h2>

<ul>
<li><p><strong>Visual Madlibs: Fill in the blank Image Generation and Question Answering.</strong>
Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg.
arXiv:1506.00278 [cs.CV].
<a href="http://arxiv.org/abs/1506.00278">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1506.00278.pdf">[PDF]</a></p></li>
<li><p><strong>ReferItGame: Referring to Objects in Photographs of Natural Scenes.</strong>
Sahar Kazemzadeh<em>, Vicente Ordonez</em>, Mark Matten, Tamara L. Berg.
Empirical Methods in Natural Language Processing (EMNLP) 2014.  Doha, Qatar.  October 2014.
<a href="http://tamaraberg.com/referitgame/">[<strong>Project Page</strong>]</a>
<a href="http://tamaraberg.com/papers/referit.pdf">[PDF]</a>
<a href="http://www.referitgame.com/">[Game]</a></p></li>
<li><p><strong>VQA: Visual Question Answering.</strong>
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh.
arXiv:1505.00468 [cs.CL]
<a href="http://arxiv.org/abs/1505.00468">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1505.00468v1.pdf">[PDF]</a></p></li>
<li><p><strong>Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering.</strong>
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu.
arXiv:1505.05612 [cs.CV].
<a href="http://arxiv.org/abs/1505.05612">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1505.05612v1.pdf">[PDF]</a></p></li>
<li><p><strong>Image Question Answering: A Visual Semantic Embedding Model and a New Dataset.</strong>
Mengye Ren, Ryan Kiros, Richard Zemel.
arXiv:1505.02074 [cs.LG].
<a href="http://arxiv.org/abs/1505.02074">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1505.02074v1.pdf">[PDF]</a></p></li>
<li><p>dataset from ("Joint Photo Stream and Blog Post Summarization and Exploration" and "Ranking and Retrieval of Image Sequences from Multiple Paragraph Queries"), CVPR 2015</p></li>
<li><p>Disney dataset (check that is same as above/hasn't changed)</p></li>
<li><p><strong>Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks.</strong>
Xiao Lin, Devi Parikh.
arXiv:1502.06108 [cs.CV].
<a href="http://arxiv.org/abs/1502.06108">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1502.06108v2.pdf">[PDF]</a></p></li>
</ul>

<h2>
<a id="5-more-possibilities" class="anchor" href="#5-more-possibilities" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>5. More Possibilities</strong>
</h2>

<ul>
<li>What, Where, Who? (ICCV 2007) \cite{fei2010whathwherewho}</li>
<li>Ramanath event-centric paper (with Fei Fei) (ICCV 2013)</li>
<li>Ontology of events and social settings (Karpathy and Fei Fei 2015)</li>
</ul>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey">Vision to Language Survey</a> is maintained by <a href="https://github.com/VisionToLanguageTeam">VisionToLanguageTeam</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

