<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Blending Vision &amp; Language in AI by VisionToLanguageTeam</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Blending Vision &amp; Language in AI</h1>
      <h2 class="project-tagline">A Survey on the Available Corpora</h2>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey" class="btn">View on GitHub</a>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="1-introduction" class="anchor" href="#1-introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>1. Introduction</strong>
</h2>

<p>Integrating vision and language has long been a dream in work on artificial intelligence (AI).
In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond.
The available corpora have played a crucial role in advancing this area of research.
In this paper we propose a set of quality metrics for evaluating and analyzing the vision-&amp;-language datasets and classify them accordingly.
Our analyses show that the most recent datasets have been using more complex language.</p>

<h2>
<a id="2-image-captioning" class="anchor" href="#2-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>2. Image Captioning</strong>
</h2>

<h3>
<a id="2-1-user-generated-captions" class="anchor" href="#2-1-user-generated-captions" aria-hidden="true"><span class="octicon octicon-link"></span></a>2-1. User-generated Captions</h3>

<ul>
<li>
<p><strong>SBU Captioned Photo Dataset</strong> (Stony Brook University) <a href="http://tlberg.cs.unc.edu/vicente/sbucaptions/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This dataset contains 1 million images with original user-generated captions, collected in the wild by systematic querying (specific terms such as objects and actions) and then filtering Flickr photos with descriptions longer than certain mean length.</p></li>
<li><p>Vicente Ordonez, Girish Kulkarni, Tamara L. Berg. s.
<em>Im2Text: Describing Images Using 1 Million Captioned Photograph.</em>
Neural Information Processing Systems(NIPS), 2011.
<a href="http://tamaraberg.com/papers/generation_nips2011.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Yahoo Flickr Creative Commons 100M Dataset (YFCC-100M)</strong> (Yahoo Lab) <a href="http://labs.yahoo.com/news/yfcc100m/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>YFCC-100M contains 100 million media objects (together with their original metadata), about 99.2 million photos
and 0.8 million videos from Flickr (taken from 2004 until early 2014), all of which are licensed as Creative Commons.</p></li>
<li><p>Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, Li-Jia Li.
<em>The New Data and New Challenges in Multimedia Research</em>.
arXiv:1503.01817 [cs.MM].
<a href="http://arxiv.org/pdf/1503.01817v1.pdf">[PDF]</a>
<a href="http://arxiv.org/abs/1503.01817">[Arxiv]</a></p></li>
</ul>
</li>
<li>
<p><strong>Déjà Images Dataset</strong> (Stony Brook University &amp; UW) <a href="http://nlclient83.cs.stonybrook.edu:8081/static/index.html">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>Déjà Images Dataset consists of 180K unique user-generated captions associated with about 4M Flickr images, where one caption is enforced to be associated with multiple images. They query Flickr for 693 of high frequency nouns and further filter captions for containing at least one verb and be "good" captions as judged by Turkers.</p></li>
<li><p>Jianfu Chen, Polina Kuznetsova, David Warren, Yejin Choi.
<em>Déjà Image-Captions: A Corpus of Expressive Image Descriptions in Repetition.</em>
North American Chapter of the Association for Computational Linguistics (NAACL), 2015.
<a href="http://www3.cs.stonybrook.edu/%7Ejianchen/papers/naacl2015.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>

<h3>
<a id="2-2-crowd-sourced-captions" class="anchor" href="#2-2-crowd-sourced-captions" aria-hidden="true"><span class="octicon octicon-link"></span></a>2-2. Crowd-sourced Captions</h3>

<ul>
<li>
<p><strong>PASCAL Dataset (1K)</strong> (UIUC) <a href="http://vision.cs.uiuc.edu/pascal-sentences/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>PASCAL is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image written by Amazon Turkers.</p></li>
<li><p>Ali Farhadi, Mohsen Hejrati, Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, David Forsyth.
<em>Every Picture Tells a Story: Generating Sentences for Images.</em>
In proceedings of European conference on Computer Vision (ECCV'10).
<a href="http://web.engr.illinois.edu/%7Emsadegh2/publications/sentence.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Flickr 8K Images</strong> (UIUC) <a href="http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This dataset consists of 8,092 Flickr images, each captioned by multiple Amazon Turkers totalling more than 40,000 image description. The focus of the dataset is on people or animals (mainly dogs) performing some specific action. </p></li>
<li><p>Cyrus Rashtchian, Peter Young, Micah Hodosh and Julia Hockenmaier.
<em>Collecting Image Annotations Using Amazon's Mechanical Turk.</em>
Proc. the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk.
<a href="http://nlp.cs.illinois.edu/HockenmaierGroup/Papers/AMT2010/W10-0721.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Flickr 30K Images</strong> (UIUC) <a href="http://shannon.cs.illinois.edu/DenotationGraph/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This dataset is an extention of Flickr 8K dataset consisting of 158,915 crowd-sourced captions which describe 31,783 images. This dataset mainly focuses on people performing everyday activities and involved in everyday events.</p></li>
<li><p>Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier.
<em>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.</em>
Transactions of the Association for Computational Linguistics 2 (2014): 67-78.
<a href="http://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Flickr 30K Entities</strong> (UIUC) <a href="http://web.engr.illinois.edu/%7Ebplumme2/Flickr30kEntities/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This datasets augments the Flickr 30K dataset with additional layers of annotation such as 244K coreference chains as
well as 276K manually annotated bounding boxes for entities.</p></li>
<li><p>Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
<em>Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.</em>
arXiv:1505.04870, 2015.
<a href="http://arxiv.org/pdf/1505.04870v1.pdf">[PDF]</a>
<a href="http://arxiv.org/abs/1505.04870">[Arxiv]</a></p></li>
</ul>
</li>
<li>
<p><strong>Richly Annotated Images</strong> (Microsoft Research) <a href="http://research.microsoft.com/en-us/downloads/b8887ebe-dc2f-4f4b-94d4-65b8432f7df4/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This work provides a set of 500 images selected from Flickr 8K dataset that are densely labeled with 100,000
textual labels (with bounding boxes and facets annotated for each object) in order to approximate gold standard visual recognition.</p></li>
<li><p>Mark Yatskar, Michel Galley, Lucy Vanderwende, and Luke Zettlemoyer.
<em>See No Evil, Say No Evil: Description Generation from Densely Labeled Images.</em>
In Third Joint Conference on Lexical and Computation Semantics (*SEM) , 2014.
<a href="http://homes.cs.washington.edu/%7Emy89/">[Code and Data]</a>
<a href="http://homes.cs.washington.edu/%7Emy89/publications/StarSem2014-SeeNoEvil.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Microsoft COCO Dataset (MS COCO)</strong> (Microsoft Research) <a href="http://mscoco.org/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>Lin et al. gathers images of complex everyday scenes which contain common objects in naturally
occurring contexts, with the goal of enhancing scene understanding. In this dataset the objects
in the scene are labeled using per-instance segmentations. In total it contains photos of 91 basic
object type with 2.5 million labeled instances in 328k images, each paired with 5 captions. This
dataset gave rise to CVPR 2015 image captioning challenge and is continuing to be a benchmark for
comparing various aspects of vision and language research.</p></li>
<li><p>Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollár.
<em>Microsoft COCO: Common Objects in Context.</em>
arXiv:1405.0312 [cs.CV].
<a href="http://arxiv.org/abs/1405.0312">[arxiv]</a></p></li>
</ul>
</li>
<li>
<p><strong>Abstract Scene Dataset</strong> (MSR, V-Tech, CMU) <a href="http://research.microsoft.com/en-us/um/people/larryz/clipart/abstract_scenes.html">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This dataset was created with the goal of representing real-world scenes by clip arts to study semantic scene understanding isolated from object recognition and segmentation issues in image processing. This dataset contains 10,020 images of children playing outdoors associated with total 60,396 descriptions.</p></li>
<li><p>C. L. Zitnick and D. Parikh.
<em>Bringing Semantics Into Focus Using Visual Abstraction.</em>
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.
<a href="http://research.microsoft.com/en-us/um/people/larryz/ZitnickParikhAbstractScenes.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>



<ul>
<li>
<p><strong>Visual and Linguistic Treebank (Visual Dependency Representations, VDR)</strong> (University of Edinburgh) <a href="http://homepages.inf.ed.ac.uk/s0128959/dataset/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This dataset consists of a set of 2,424 images with 3 one-sentence captions sourced to Amazon Turkers describing the main action in the photo (for the 10 types of actions in the set) and one sentence describing the other regions not involved in the action.</p></li>
<li><p>Desmond Elliott and Frank Keller.
<em>Image Description using Visual Dependency Representations.</em>
EMNLP 2013.
<a href="http://aclweb.org/anthology/D/D13/D13-1128.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>

<h2>
<a id="3-video-captioning" class="anchor" href="#3-video-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>3. Video Captioning</strong>
</h2>

<ul>
<li>
<p><strong>Multilingual Corpus of Robocup Soccer Events</strong> (UT Austin) <a href="http://www.cs.utexas.edu/%7Eml/clamp/sportscasting/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This dataset is a multilingual corpus of Robocup soccer events (e.g., kicking and passing) aligned with human-generated comments in Korean and English. It contains total of four games, 2,036 English and 1,999 Korean comments which are very short in length and limited in vocabulary.</p></li>
<li><p>David L. Chen, Joohyun Kim, Raymond J. Mooney.
<em>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language.</em>
In Journal of Artificial Intelligence Research (JAIR) , 37, pages 397-435, 2010.
<a href="http://dl.acm.org/citation.cfm?id=1861761">[ACM]</a>
<a href="https://www.jair.org/media/2962/live-2962-4903-jair.pdf">[PDF]</a>
<a href="http://www.jair.org/papers/paper2962.html">[JAIR link]</a>    </p></li>
</ul>
</li>
<li>
<p><strong>Short Videos Described with Sentences</strong> (Purdue University) <a href="http://haonanyu.com/research/acl2013/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This work provides a dataset for learning word meanings from short video clips which are manually annotated with one or more sentences. Their dataset manually annotates 3-5 second long 61 video clips with sentences which are highly resctricted in terms of grammar and language.</p></li>
<li><p>H. Yu and J. M. Siskind.
<em>Grounded Language Learning from Video Described with Sentences.</em>
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013, <em>best paper award</em>.
<a href="http://haonanyu.com/wp-content/uploads/2013/05/yu13.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>





<ul>
<li>
<p><strong>Microsoft Research Video Description Corpus (MS VDC)</strong> (UT Austin &amp; MSR) <a href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/">[<strong>Project Page</strong>]</a>
<a href="http://research.microsoft.com/en-us/downloads/38cf15fd-b8df-477e-a4e4-a4680caa75af/default.aspx">[Data]</a></p>

<ul>
<li><p>MS VDC contains parallel descriptions (85,550 English ones) of 2,089 short video snippets (10-25 seconds long). The descriptions are one sentence summary about the action or event in the video as described by Amazon Turkers. In this dataset both paraphrase and bilingual alternatives are captured, hence, the dataset can be useful translation, paraphrasing, and video description purposes.</p></li>
<li><p>David L. Chen and William B. Dolan.
<em>Collecting Highly Parallel Data for Paraphrase Evaluation.</em>
Annual Meetings of the Association for Computational Linguistics (ACL), 2011.
<a href="http://www.cs.utexas.edu/users/ml/papers/chen.acl11.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>MPII Cooking Activities Dataset</strong> (Max Planck Institute for Informatics) <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/human-activity-recognition/mpii-cooking-activities-dataset/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>This is a video corpus that annotates 41 low-level different cooking activities (e.g., "separating eggs" or "cutting veggies") in 212 video segments with average 4.5 minutes length. This corpus specifically annotates participating objects in activities (e.g., TAKE OUT activity has [HAND, KNIFE, DRAWER] participants).</p></li>
<li><p>M. Rohrbach, S. Amin, M. Andriluka and B. Schiele.
<em>A Database for Fine Grained Activity Detection of Cooking Activities.</em>
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June, (2012).
<a href="https://www.mpi-inf.mpg.de/fileadmin/inf/d2/amin/rohrbach12cvpr.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Saarbrucken Corpus of Textually Annotated Scenes (TACOS Corpus)</strong> (Saarland University &amp;  Max Planck Institute for Informatics) <a href="http://www.coli.uni-saarland.de/projects/smile/page.php?id=tacos">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>The TACOS dataset extends MPII Cooking Activities Dataset by aligning textual descriptions with video segments.</p></li>
<li><p>Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.
<em>Grounding Action Descriptions in Videos.</em>
TACL 2013.
<a href="http://www.aclweb.org/anthology/Q13-1003">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>Instructional Video Captions</strong></p>

<ul>
<li><p>Sore recent works have proposed unsupervised learning algorithms for automatically associating sentences in a document with video segments.
Malmaud et al. focus on the cooking domain, aligning written recipe steps with a videos.
Naim et al. align the natural language instructions for biological experiments in "wet laboratories" with recorded videos of people performing these experiments.</p></li>
<li><p>What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy.
NAACL 2015.
<a href="http://www.cs.ubc.ca/%7Emurphyk/Papers/naacl15.pdf">[PDF]</a></p></li>
<li><p>Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments.
I. Naim, Y. Song, Q. Liu, L. Huang, H. Kautz, J. Luo, and D. Gildea. 
Proc. NAACL 2015.
<a href="http://acl.cs.qc.edu/%7Elhuang/papers/naim-video.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>





<h2>
<a id="4-beyond-image-captioning" class="anchor" href="#4-beyond-image-captioning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>4. Beyond Image Captioning</strong>
</h2>

<ul>
<li>
<p><strong>Visual MadLibs</strong> (UNC)</p>

<ul>
<li><p>One sentence here</p></li>
<li><p>Visual Madlibs: Fill in the blank Image Generation and Question Answering.
Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg.
arXiv:1506.00278 [cs.CV].
<a href="http://arxiv.org/abs/1506.00278">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1506.00278.pdf">[PDF]</a></p></li>
</ul>
</li>
<li>
<p><strong>ReferIt dataset</strong> (UNC) <a href="http://tamaraberg.com/referitgame/">[<strong>Project Page</strong>]</a></p>

<ul>
<li><p>One sentence here</p></li>
<li><p>ReferItGame: Referring to Objects in Photographs of Natural Scenes.
Sahar Kazemzadeh<em>, Vicente Ordonez</em>, Mark Matten, Tamara L. Berg.
Empirical Methods in Natural Language Processing (EMNLP) 2014.  Doha, Qatar.  October 2014.
<a href="http://tamaraberg.com/papers/referit.pdf">[PDF]</a>
<a href="http://www.referitgame.com/">[Game]</a></p></li>
</ul>
</li>
<li>
<p><strong>Visual Question Answering (VQA) Dataset</strong> (Microsoft Research)</p>

<ul>
<li><p>One sentence here</p></li>
<li><p>VQA: Visual Question Answering.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh.
arXiv:1505.00468 [cs.CL]
<a href="http://arxiv.org/abs/1505.00468">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1505.00468v1.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>



<ul>
<li>
<p><strong>Toronto COCO-QA Dataset</strong> (University of Toronto)</p>

<ul>
<li><p>This is a simpler VQA dataset where the questions are automatically generated from image captions of MS COCO dataset. This dataset has total 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations.</p></li>
<li><p>Image Question Answering: A Visual Semantic Embedding Model and a New Dataset.
Mengye Ren, Ryan Kiros, Richard Zemel.
arXiv:1505.02074 [cs.LG].
<a href="http://arxiv.org/abs/1505.02074">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1505.02074v1.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>



<ul>
<li>
<p><strong>Fill-in-the-blank (FITB) &amp; Visual Paraphrasing (VP) Dataset</strong> (Virginia Tech)</p>

<ul>
<li><p>One sentence here</p></li>
<li><p>Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks.
Xiao Lin, Devi Parikh.
arXiv:1502.06108 [cs.CV].
<a href="http://arxiv.org/abs/1502.06108">[Arxiv]</a>
<a href="http://arxiv.org/pdf/1502.06108v2.pdf">[PDF]</a></p></li>
</ul>
</li>
</ul>



      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/VisionToLanguageTeam/Vision-To-Language-Survey">Blending Vision &amp; Language in AI</a> is maintained by <a href="https://github.com/VisionToLanguageTeam">VisionToLanguageTeam</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

