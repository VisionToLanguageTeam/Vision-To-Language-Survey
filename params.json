{"name":"Vision to Language Survey","tagline":"","body":"## **1. Introduction**\r\n\r\nThe past year has seen an explosion in work on vision to language. In the 2012 ImageNet competition, a neural network model (`SuperVision') won at the task of object  classification.\r\nIn the 2013 ImageNet competition, a neural network model pioneered by UC Berkeley won at the task of object detection.\r\nThis unified two otherwise disparate computer vision fields under the same general modelling technique.  It became possible to train object detectors reliably, when provided with enough training data.\r\n\r\n## **2. Video Captioning**\r\n\r\n* **Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language**.\r\nDavid L. Chen, Joohyun Kim, Raymond J. Mooney.\r\nIn Journal of Artificial Intelligence Research (JAIR) , 37, pages 397-435, 2010.\r\n[[**Project Page**]](http://www.cs.utexas.edu/~ml/clamp/sportscasting/)\r\n[[ACM]](http://dl.acm.org/citation.cfm?id=1861761)\r\n[[PDF]](https://www.jair.org/media/2962/live-2962-4903-jair.pdf)\r\n[[JAIR link]](http://www.jair.org/papers/paper2962.html)\r\n\r\n* **Grounded Language Learning from Video Described with Sentences**.\r\nH. Yu and J. M. Siskind.\r\nIn Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013, *best paper award*.\r\n[[**Project Page**]](http://haonanyu.com/research/acl2013/)\r\n[[PDF]](http://haonanyu.com/research/acl2013/)\r\n\r\n* **Story-Driven Summarization for Egocentric Video**.\r\nZheng Lu and Kristen Grauman.\r\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Portland, OR, June 2013.\r\n[[**Project Page**]](http://vision.cs.utexas.edu/projects/egocentric/storydriven.html)\r\n[[PDF]](http://www.cs.utexas.edu/~grauman/papers/lu-grauman-cvpr2013.pdf)\r\n\r\n* **Movie Script Summarization as Graph-based Scene Extraction**.\r\nPhilip John Gorinski and Mirella Lapata.\r\nProc. Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL (NAACL 2015), pages 1066–1076.\r\nMay 31 – June 5, 2015.\r\n[[PDF]](http://www.aclweb.org/anthology/N/N15/N15-1113.pdf)\r\n\r\n* **Collecting Highly Parallel Data for Paraphrase Evaluation**\r\nDavid L. Chen and William B. Dolan.\r\nAnnual Meetings of the Association for Computational Linguistics (ACL), 2011.\r\n[[**Project Page**]](http://www.cs.utexas.edu/users/ml/clamp/videoDescription/)\r\n[[PDF]](http://www.cs.utexas.edu/users/ml/papers/chen.acl11.pdf)\r\n\r\n* **What’s Cookin’? Interpreting Cooking Videos using Text, Speech and Vision**.\r\nJonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy.\r\nNAACL 2015.\r\n[[PDF]](http://www.cs.ubc.ca/~murphyk/Papers/naacl15.pdf)\r\n\r\n* **Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments**.\r\nI. Naim, Y. Song, Q. Liu, L. Huang, H. Kautz, J. Luo, and D. Gildea. \r\nProc. NAACL 2015.\r\n[[PDF]](http://acl.cs.qc.edu/~lhuang/papers/naim-video.pdf)\r\n\r\n* **Grounding Action Descriptions in Videos**.\r\nMichaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.\r\nTACL 2013.\r\n[[PDF]](http://www.aclweb.org/anthology/Q13-1003)\r\n\r\n* **Translating Videos to Natural Language Using Deep Recurrent Neural Networks**.\r\nSubhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko.\r\nNorth American Chapter of the Association for Computational Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)\r\n[[PDF]](https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_NAACL15.pdf)\r\n[[Code]](https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube)\r\n\r\n* **Book2movie: Aligning video scenes with book chapters.**\r\nTapaswi, Makarand, Martin Bäuml, and Rainer Stiefelhagen.\r\nProc. the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\r\n[[PDF]](https://cvhci.anthropomatik.kit.edu/~mtapaswi/papers/CVPR2015.pdf)\r\n\r\n## **3. Image Captioning**\r\n\r\n### User-generated\r\n\r\n* **SBU Captioned Photo Dataset**: \r\n  Im2Text: Describing Images Using 1 Million Captioned Photograph.\r\n  Vicente Ordonez, Girish Kulkarni, Tamara L. Berg. s.\r\n  Neural Information Processing Systems(NIPS), 2011.\r\n  [[**Project Page**]](http://tlberg.cs.unc.edu/vicente/sbucaptions/)\r\n  [[PDF]](http://tamaraberg.com/papers/generation_nips2011.pdf)\r\n\r\n* **Flickr's 100 million images dataset 2012 (YFCC100M)**\r\n  [[**Project Page**]](http://labs.yahoo.com/news/yfcc100m/)\r\n\r\n### Captioned by Crowd\r\n\r\n* **PASCAL Sentences Dataset (1K)**\r\n\r\n* **Flickr 8K**\r\n  [[**Project Page**]](http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html)\r\n\r\n* **Flickr30K**:\r\n  From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.\r\n  Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier.\r\n  Transactions of the Association for Computational Linguistics 2 (2014): 67-78.\r\n  [[**Project Page**]](http://shannon.cs.illinois.edu/DenotationGraph/)\r\n  [[PDF]](http://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf)\r\n\r\n* **Flickr30K Entities**:\r\n  Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.\r\n  Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.\r\n  arXiv:1505.04870, 2015.\r\n  [[**Project Page**]](http://web.engr.illinois.edu/~bplumme2/Flickr30kEntities/)\r\n  [[PDF]](http://arxiv.org/pdf/1505.04870v1.pdf)\r\n  [[Arxiv]](http://arxiv.org/abs/1505.04870)\r\n\r\n* **Microsoft Research Dense Visual Annotation Corpus**:\r\n  See No Evil, Say No Evil: Description Generation from Densely Labeled Images.\r\n  Mark Yatskar, Michel Galley, Lucy Vanderwende, and Luke Zettlemoyer.\r\n  In Third Joint Conference on Lexical and Computation Semantics (\\*SEM) , 2014.\r\n  [[**Project Page**]](http://research.microsoft.com/en-us/downloads/b8887ebe-dc2f-4f4b-94d4-65b8432f7df4/)\r\n  [[Code and Data]](http://homes.cs.washington.edu/~my89/)\r\n  [[PDF]](http://homes.cs.washington.edu/~my89/publications/StarSem2014-SeeNoEvil.pdf)\r\n\r\n* **COCO Dataset**\r\n\r\n* **Image Description using Visual Dependency Representations.**\r\n  Desmond Elliott and Frank Keller.\r\n  EMNLP 2013.\r\n  [[PDF]](http://aclweb.org/anthology/D/D13/D13-1128.pdf)\r\n\r\n### Already Captioned\r\n\r\n* Yejin's new dataset (expressive language) - NAACL 2015; Deja Image Captions\r\n  --> \\cite{chen2015deja}\t\r\n\r\n## **4. Beyond Image Captioning**\r\n\r\n* Bergs' MadLibs (and QA?) http://arxiv.org/pdf/1506.00278.pdf \r\n  \\cite{yu2015vml}\r\n\r\n* Bergs' ReferIt dataset (EMNLP 2014) http://tamaraberg.com/referitgame/\r\n  \\cite{kazemzadeh2014referitgame}\r\n\r\n* VQA (V. tech) - ICCV 2015 \\cite{antol2015vqa}\r\n\t    -- 10K sampled (somehow) from MS COCO\r\n\r\n* mQA (might not be released yet?) Baidu - 2015 - captions converted to QA http://arxiv.org/pdf/1505.05612.pdf, NIPS 2015\r\n\r\n* The \"Image Question Answering\" dataset from Toronto:\r\n  \\cite{ren2015imageqa}\r\n\r\n* dataset from (\"Joint Photo Stream and Blog Post Summarization and Exploration\" and \"Ranking and Retrieval of Image Sequences from Multiple Paragraph Queries\"), CVPR 2015\r\n\r\n* Disney dataset (check that is same as above/hasn't changed)\r\n\r\n* Devi Parikh & students: fill-in-the-blanks and visual paraphrasing (page under constructions, datasets and code will be online here: https://filebox.ece.vt.edu/~linxiao/imagine/), CVPR 2015 \\cite{lin2015imagine}\r\n\r\n## **5. More Possibilities**\r\n\r\n* What, Where, Who? (ICCV 2007) \\cite{fei2010whathwherewho}\r\n* Ramanath event-centric paper (with Fei Fei) (ICCV 2013)\r\n* Ontology of events and social settings (Karpathy and Fei Fei 2015)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}