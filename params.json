{"name":"Blending Vision & Language in AI: A Survey on the Available Corpora","tagline":"","body":"## **1. Introduction**\r\n\r\nIntegrating vision and language has long been a dream in work on artificial intelligence (AI).\r\nIn the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond.\r\nThe available corpora have played a crucial role in advancing this area of research.\r\nIn this paper we propose a set of quality metrics for evaluating and analyzing the vision-&-language datasets and classify them accordingly.\r\nOur analyses show that the most recent datasets have been using more complex language.\r\n\r\n## **2. Image Captioning**\r\n\r\n### User-generated Captions\r\n\r\n* **SBU Captioned Photo Dataset** (Stony Brook University)\r\n\r\n  - This dataset contains 1 million images with original user-generated captions, collected in the wild by systematic querying (specific terms such as objects and actions) and then filtering Flickr photos with descriptions longer than certain mean length.\r\n\r\n  - Vicente Ordonez, Girish Kulkarni, Tamara L. Berg. s.\r\n    *Im2Text: Describing Images Using 1 Million Captioned Photograph.*\r\n    Neural Information Processing Systems(NIPS), 2011.\r\n    [[**Project Page**]](http://tlberg.cs.unc.edu/vicente/sbucaptions/)\r\n    [[PDF]](http://tamaraberg.com/papers/generation_nips2011.pdf)\r\n\r\n* **Flickr's 100 million images dataset 2012 (YFCC100M)**\r\n\r\n  - YFCC100M contains 100 million media objects (together with their original metadata), about 99.2 million photos\r\nand 0.8 million videos from Flickr (taken from 2004 until early 2014), all of which are licensed as Creative Commons.\r\n\r\n  - Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, Li-Jia Li.\r\n    *The New Data and New Challenges in Multimedia Research*.\r\n    arXiv:1503.01817 [cs.MM].\r\n    [[PDF]](http://arxiv.org/pdf/1503.01817v1.pdf)\r\n    [[Arxiv]](http://arxiv.org/abs/1503.01817)\r\n    [[**Project Page**]](http://labs.yahoo.com/news/yfcc100m/)\r\n\r\n* **Déjà-Image Captions Dataset** (Stony Brook University & UW)\r\n\r\n  - The deja-image captions data set consists of naturally occurring captions for images where the captions were found to be nearly identical for multiple images; the language is rich and varied, with 17 percent in a random sample found to be figurative. Each image is augmented with a set of retrieved candidates from the full set, that were vetted by AMT workers to also be good captions for the image.\r\n\r\n  - Déjà Image-Captions: A Corpus of Expressive Image Descriptions in Repetition.\r\n    Jianfu Chen, Polina Kuznetsova, David Warren, Yejin Choi.\r\n    North American Chapter of the Association for Computational Linguistics (NAACL), 2015.\r\n    [[**Project Page**]](http://nlclient83.cs.stonybrook.edu:8081/static/index.html)\r\n    [[PDF]](http://www3.cs.stonybrook.edu/~jianchen/papers/naacl2015.pdf)\r\n\r\n### Crowd-sourced Captions\r\n\r\n* **PASCAL Sentences Dataset (1K)**\r\n\r\n* **Flickr 8K** (UIUC)\r\n\r\n  -  This dataset consists of 8,092 Flickr images, each captioned by multiple Amazon Turkers totalling more than 40,000 image description. The focus of the dataset is on people or animals (mainly dogs) performing some specific action. \r\n\r\n  - Collecting Image Annotations Using Amazon's Mechanical Turk.\r\n    Cyrus Rashtchian, Peter Young, Micah Hodosh and Julia Hockenmaier.\r\n    Proc. the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk.\r\n    [[**Project Page**]](http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html)\r\n\r\n* **Flickr30K** (UIUC)\r\n\r\n  - This dataset is an extention of Flickr 8K dataset consisting of 158,915 crowd-sourced captions which describe 31,783 images. This dataset mainly focuses on people performing everyday activities and involved in everyday events.\r\n\r\n  - From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.\r\n    Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier.\r\n    Transactions of the Association for Computational Linguistics 2 (2014): 67-78.\r\n    [[**Project Page**]](http://shannon.cs.illinois.edu/DenotationGraph/)\r\n    [[PDF]](http://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf)\r\n\r\n* **Flickr30K Entities** (UIUC)\r\n\r\n  - One sentence here.\r\n  \r\n  - Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models.\r\n    Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.\r\n    arXiv:1505.04870, 2015.\r\n    [[**Project Page**]](http://web.engr.illinois.edu/~bplumme2/Flickr30kEntities/)\r\n    [[PDF]](http://arxiv.org/pdf/1505.04870v1.pdf)\r\n    [[Arxiv]](http://arxiv.org/abs/1505.04870)\r\n\r\n* **Microsoft Research Dense Visual Annotation Corpus** (Microsoft Research)\r\n\r\n  - One sentence here.\r\n\r\n  - See No Evil, Say No Evil: Description Generation from Densely Labeled Images.\r\n    Mark Yatskar, Michel Galley, Lucy Vanderwende, and Luke Zettlemoyer.\r\n    In Third Joint Conference on Lexical and Computation Semantics (\\*SEM) , 2014.\r\n    [[**Project Page**]](http://research.microsoft.com/en-us/downloads/b8887ebe-dc2f-4f4b-94d4-65b8432f7df4/)\r\n    [[Code and Data]](http://homes.cs.washington.edu/~my89/)\r\n    [[PDF]](http://homes.cs.washington.edu/~my89/publications/StarSem2014-SeeNoEvil.pdf)\r\n\r\n* **COCO Dataset** (Microsoft Research)\r\n\r\n* **Visual and Linguistic Treebank** (University of Edinburgh)\r\n\r\n  - The Visual and Linguistic Treebank consists of a set of 2,424 images with 3 descriptions for each image sourced by asking AMT workers to write one sentence describing the main action in the photo (there are only 10 types of actions in the set) and one sentence describing the other regions not involved in the action. For a set of 341 images, AMT workers additionally identified the bounding boxes of the objects in the description and created the visual dependency representation for each image, which represents the relative positions of the various objects in the image.\r\n\r\n  - Image Description using Visual Dependency Representations.\r\n    Desmond Elliott and Frank Keller.\r\n    EMNLP 2013.\r\n    [[PDF]](http://aclweb.org/anthology/D/D13/D13-1128.pdf)\r\n## **3. Video Captioning**\r\n\r\n* **Robocup Soccer Dataset** (UT Austin)\r\n\r\n  - There have been many efforts on automatic sportscasting of simulated soccer games. The most recent dataset for training purposes in this domain is a multilingual corpus of Robocup soccer events aligned with human-generated comments in Korean and English.\r\n\r\n  - Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language.\r\n    David L. Chen, Joohyun Kim, Raymond J. Mooney.\r\n    In Journal of Artificial Intelligence Research (JAIR) , 37, pages 397-435, 2010.\r\n    [[**Project Page**]](http://www.cs.utexas.edu/~ml/clamp/sportscasting/)\r\n    [[ACM]](http://dl.acm.org/citation.cfm?id=1861761)\r\n    [[PDF]](https://www.jair.org/media/2962/live-2962-4903-jair.pdf)\r\n    [[JAIR link]](http://www.jair.org/papers/paper2962.html)    \r\n\r\n* **Video-with-Sentences Dataset** (Purdue University)\r\n\r\n  - This work provides a dataset for learning word meanings from short video clips which are manually annotated with one or more sentences. Their dataset manually annotates 3-5 second long 61 video clips with sentences which are highly resctricted in terms of grammar and language.\r\n\r\n  - Grounded Language Learning from Video Described with Sentences.\r\n    H. Yu and J. M. Siskind.\r\n    In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013, *best paper award*.\r\n    [[**Project Page**]](http://haonanyu.com/research/acl2013/)\r\n    [[PDF]](http://haonanyu.com/wp-content/uploads/2013/05/yu13.pdf)\r\n\r\n<!--[No longer included]\r\n* Story-Driven Summarization for Egocentric Video.\r\n    Zheng Lu and Kristen Grauman.\r\n    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Portland, OR, June 2013.\r\n    [[**Project Page**]](http://vision.cs.utexas.edu/projects/egocentric/storydriven.html)\r\n    [[PDF]](http://www.cs.utexas.edu/~grauman/papers/lu-grauman-cvpr2013.pdf)\r\n-->\r\n\r\n<!-- [This work is on movie \"script\" Summarization, has nothing to do with videos]\r\n* **Movie Script Summarization as Graph-based Scene Extraction**.\r\nPhilip John Gorinski and Mirella Lapata.\r\nProc. Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL (NAACL 2015), pages 1066–1076.\r\nMay 31 – June 5, 2015.\r\n[[PDF]](http://www.aclweb.org/anthology/N/N15/N15-1113.pdf)\r\n-->\r\n\r\n* **Multilingual Parallel Video Description Dataset** (UT Austin & MSR)\r\n\r\n  - One sentence here.\r\n\r\n  - Collecting Highly Parallel Data for Paraphrase Evaluation.\r\n    David L. Chen and William B. Dolan.\r\n    Annual Meetings of the Association for Computational Linguistics (ACL), 2011.\r\n    [[**Project Page**]](http://www.cs.utexas.edu/users/ml/clamp/videoDescription/)\r\n    [[PDF]](http://www.cs.utexas.edu/users/ml/papers/chen.acl11.pdf)\r\n\r\n* **How-To Video Captions**\r\n\r\n  - One sentence here.\r\n\r\n  - What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision.\r\n    Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy.\r\n    NAACL 2015.\r\n    [[PDF]](http://www.cs.ubc.ca/~murphyk/Papers/naacl15.pdf)\r\n\r\n  - Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments.\r\n    I. Naim, Y. Song, Q. Liu, L. Huang, H. Kautz, J. Luo, and D. Gildea. \r\n    Proc. NAACL 2015.\r\n    [[PDF]](http://acl.cs.qc.edu/~lhuang/papers/naim-video.pdf)\r\n\r\n*  **Saarbrucken Corpus of Textually Annotated Scenes (TACOS Corpus)** (Saarland University & \u0005 Max Planck Institute for Informatics)\r\n\r\n   - One sentence here.\r\n\r\n   - Grounding Action Descriptions in Videos.\r\n     Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal.\r\n     TACL 2013.\r\n     [[PDF]](http://www.aclweb.org/anthology/Q13-1003)\r\n\r\n<!-- [This work uses Microsoft Research Video Description Corpus, so we will not mention it in corpora, however, we can cite it in intro/elsewhere as the most recent video description paper.]\r\n* **Translating Videos to Natural Language Using Deep Recurrent Neural Networks**.\r\nSubhashini Venugopalan, Huijun Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko.\r\nNorth American Chapter of the Association for Computational Linguistics, Denver, Colorado, June 2015. (NAACL-HLT 2015)\r\n[[PDF]](https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_NAACL15.pdf)\r\n[[Code]](https://github.com/vsubhashini/caffe/tree/recurrent/examples/youtube)\r\n-->\r\n\r\n<!-- [!! I propose not to include this work after reading the paper now. Their dataset is not gold standard, i.e., their alignments are error-prone and I wouldn't list among the other available datasets, but approaches.]\r\n* **Book2movie Dataset** (Karlsruhe Institute of Technology)\r\n\r\n  - One sentence: This dataset captures the alignment of a movie scene with a book chapter and can be viewed as an example of user-generated captioning, though the movies were created as the images for pre-exsting \"captions\", i.e. novel text.\r\n\r\n  - Book2movie: Aligning video scenes with book chapters.**\r\n    Tapaswi, Makarand, Martin Bäuml, and Rainer Stiefelhagen.\r\n    Proc. the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\r\n    [[PDF]](https://cvhci.anthropomatik.kit.edu/~mtapaswi/papers/CVPR2015.pdf)\r\n-->\r\n\r\n\r\n\r\n## **4. Beyond Image Captioning**\r\n\r\n* **Visual MadLibs** (UNC)\r\n\r\n  - One sentence here\r\n\r\n  - Visual Madlibs: Fill in the blank Image Generation and Question Answering.\r\n    Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg.\r\n    arXiv:1506.00278 [cs.CV].\r\n    [[Arxiv]](http://arxiv.org/abs/1506.00278)\r\n    [[PDF]](http://arxiv.org/pdf/1506.00278.pdf)\r\n\r\n* **ReferIt dataset** (UNC)\r\n\r\n  - One sentence here\r\n\r\n  - ReferItGame: Referring to Objects in Photographs of Natural Scenes.\r\n    Sahar Kazemzadeh*, Vicente Ordonez*, Mark Matten, Tamara L. Berg.\r\n    Empirical Methods in Natural Language Processing (EMNLP) 2014.  Doha, Qatar.  October 2014.\r\n    [[**Project Page**]](http://tamaraberg.com/referitgame/)\r\n    [[PDF]](http://tamaraberg.com/papers/referit.pdf)\r\n    [[Game]](http://www.referitgame.com/)\r\n\r\n* **Visual Question Answering (VQA) Dataset** (Microsoft Research)\r\n\r\n  - One sentence here\r\n\r\n  - VQA: Visual Question Answering.**\r\n    Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh.\r\n    arXiv:1505.00468 [cs.CL]\r\n    [[Arxiv]](http://arxiv.org/abs/1505.00468)\r\n    [[PDF]](http://arxiv.org/pdf/1505.00468v1.pdf)\r\n\r\n<!-- [Not sure if we keep this]\r\n(16) mQA (might not be released yet?) Baidu - 2015 - captions converted to QA http://arxiv.org/pdf/1505.05612.pdf, NIPS 2015\t\r\n* **Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering.**\r\n  Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu.\r\n  arXiv:1505.05612 [cs.CV].\r\n  [[Arxiv]](http://arxiv.org/abs/1505.05612)\r\n  [[PDF]](http://arxiv.org/pdf/1505.05612v1.pdf)\r\n-->\r\n\r\n* **COCO-QA Dataset** (University of Toronto)\r\n\r\n  -  This is a simpler VQA dataset where the questions are automatically generated from image captions of MS COCO dataset. This dataset has total 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations.\r\n\r\n  - Image Question Answering: A Visual Semantic Embedding Model and a New Dataset.\r\n    Mengye Ren, Ryan Kiros, Richard Zemel.\r\n    arXiv:1505.02074 [cs.LG].\r\n    [[Arxiv]](http://arxiv.org/abs/1505.02074)\r\n    [[PDF]](http://arxiv.org/pdf/1505.02074v1.pdf)\r\n\r\n<!--\r\n* dataset from (\"Joint Photo Stream and Blog Post Summarization and Exploration\" and \"Ranking and Retrieval of Image Sequences from Multiple Paragraph Queries\"), CVPR 2015\r\n\r\n* Disney dataset (check that is same as above/hasn't changed)\r\n-->\r\n\r\n* **Fill-in-the-blank (FITB) & Visual Paraphrasing (VP) Dataset** (Virginia Tech)\r\n\r\n  - One sentence here\r\n\r\n  - Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks.\r\n    Xiao Lin, Devi Parikh.\r\n    arXiv:1502.06108 [cs.CV].\r\n    [[Arxiv]](http://arxiv.org/abs/1502.06108)\r\n    [[PDF]](http://arxiv.org/pdf/1502.06108v2.pdf)\r\n\r\n<!-- Do it later\r\n## **5. More Possibilities**\r\n* What, Where, Who? (ICCV 2007) \\cite{fei2010whathwherewho}\r\n* Ramanath event-centric paper (with Fei Fei) (ICCV 2013)\r\n* Ontology of events and social settings (Karpathy and Fei Fei 2015)\r\n-->","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}